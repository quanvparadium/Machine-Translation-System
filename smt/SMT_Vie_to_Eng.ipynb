{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e4d8d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aacee67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_stores = {'en_train': [], 'en_validation': [], 'en_test': [], \n",
    "                    'vi_train': [], 'vi_validation': [], 'vi_test': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b332f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in tokenized_stores:\n",
    "    file_name = \"D:/Statistical-Machine-Translation-master/data/\" + str(key)[3:] + \".\" + str(key)[0:2]\n",
    "    load = open(file_name, 'r', encoding='utf-8')\n",
    "    sentences = load.read().split('\\n')\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        token_store = sentence.split(' ')\n",
    "        tokenized_stores[key].append(token_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cef74d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', '4', 'minutes', ',', 'atmospheric', 'chemist', 'Rachel', 'Pike', 'provides', 'a', 'glimpse', 'of', 'the', 'massive', 'scientific', 'effort', 'behind', 'the', 'bold', 'headlines', 'on', 'climate', 'change', ',', 'with', 'her', 'team', '--', 'one', 'of', 'thousands', 'who', 'contributed', '--', 'taking', 'a', 'risky', 'flight', 'over', 'the', 'rainforest', 'in', 'pursuit', 'of', 'data', 'on', 'a', 'key', 'molecule', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_stores['en_train'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfc50ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Trong', '4', 'phút', ',', 'chuyên', 'gia', 'hoá', 'học', 'khí', 'quyển', 'Rachel', 'Pike', 'giới', 'thiệu', 'sơ', 'lược', 'về', 'những', 'nỗ', 'lực', 'khoa', 'học', 'miệt', 'mài', 'đằng', 'sau', 'những', 'tiêu', 'đề', 'táo', 'bạo', 'về', 'biến', 'đổi', 'khí', 'hậu', ',', 'cùng', 'với', 'đoàn', 'nghiên', 'cứu', 'của', 'mình', '--', 'hàng', 'ngàn', 'người', 'đã', 'cống', 'hiến', 'cho', 'dự', 'án', 'này', '--', 'một', 'chuyến', 'bay', 'mạo', 'hiểm', 'qua', 'rừng', 'già', 'để', 'tìm', 'kiếm', 'thông', 'tin', 'về', 'một', 'phân', 'tử', 'then', 'chốt', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_stores['vi_train'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5200cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133318\n"
     ]
    }
   ],
   "source": [
    "train_size = len(tokenized_stores['vi_train'])\n",
    "dev_size = len(tokenized_stores['vi_validation'])\n",
    "test_size = len(tokenized_stores['vi_test'])\n",
    "print(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a94b8a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Words:\n",
      "> English: 54418\n",
      "> Vietnamese: 25763\n"
     ]
    }
   ],
   "source": [
    "# making the vocabulary\n",
    "\n",
    "en_words = {}\n",
    "vi_words = {}\n",
    "\n",
    "for key in tokenized_stores:\n",
    "    if str(key)[0] == 'e':\n",
    "        # creating en_words\n",
    "        for sentence in tokenized_stores[key]:\n",
    "            for word in sentence:\n",
    "                if word in en_words:\n",
    "                    en_words[word] += 1\n",
    "                else:\n",
    "                    en_words[word] = 1\n",
    "    else:\n",
    "        # creating vi_words\n",
    "        for sentence in tokenized_stores[key]:\n",
    "            for word in sentence:\n",
    "                if word in vi_words:\n",
    "                    vi_words[word] += 1\n",
    "                else:\n",
    "                    vi_words[word] = 1\n",
    "                    \n",
    "en_vocab = len(en_words)\n",
    "vi_vocab = len(vi_words)\n",
    "print(\"Number of Unique Words:\")\n",
    "print(\"> English:\", str(en_vocab))\n",
    "print(\"> Vietnamese:\", str(vi_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd35aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the 't'\n",
    "t = {}\n",
    "# usage: t[('EN_word', 'VI_word')] = probability of EN_Word given VI_word\n",
    "uniform = 1 / (en_vocab * vi_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca60ad99",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 0\n",
    "max_iters = 25\n",
    "\n",
    "fine_tune = 1\n",
    "has_converged = False\n",
    "\n",
    "while n_iters < max_iters and has_converged == False:\n",
    "    has_converged = True\n",
    "    max_change = -1\n",
    "\n",
    "    n_iters += 1\n",
    "    count = {}\n",
    "    total = {}\n",
    "    for index in range(train_size):\n",
    "        s_total = {}\n",
    "        for vi_word in tokenized_stores['vi_train'][index]:\n",
    "            s_total[vi_word] = 0\n",
    "            for en_word in tokenized_stores['en_train'][index]:\n",
    "                if (vi_word, en_word) not in t:\n",
    "                    t[(vi_word, en_word)] = uniform\n",
    "                s_total[vi_word] += t[(vi_word, en_word)]\n",
    "\n",
    "        for vi_word in tokenized_stores['vi_train'][index]:\n",
    "            for en_word in tokenized_stores['en_train'][index]:\n",
    "                if (vi_word, en_word) not in count:\n",
    "                    count[(vi_word, en_word)] = 0\n",
    "                count[(vi_word, en_word)] += (t[(vi_word, en_word)] / s_total[vi_word])\n",
    "\n",
    "                if en_word not in total:\n",
    "                    total[en_word] = 0\n",
    "                total[en_word] += (t[(vi_word, en_word)] / s_total[vi_word])\n",
    "\n",
    "    # estimating the probabilities\n",
    "\n",
    "    if fine_tune == 0:\n",
    "        updated = {}\n",
    "        # train for all valid word pairs s.t count(vi_word, en_word) > 0\n",
    "        for index in range(train_size):\n",
    "            for en_word in tokenized_stores['en_train'][index]:\n",
    "                for vi_word in tokenized_stores['vi_train'][index]:\n",
    "                    if (vi_word, en_word) in updated:\n",
    "                        continue\n",
    "                    updated[(vi_word, en_word)] = 1\n",
    "                    if abs(t[(vi_word, en_word)] - count[(vi_word, en_word)] / total[en_word]) > 0.01:\n",
    "                        has_converged = False\n",
    "                        max_change = max(max_change, abs(t[(vi_word, en_word)] - count[(vi_word, en_word)] / total[en_word]))\n",
    "                    t[(vi_word, en_word)] = count[(vi_word, en_word)] / total[en_word]\n",
    "\n",
    "    elif fine_tune == 1:\n",
    "        # train it only for 1000 most frequent words in English and Hindi\n",
    "        max_words = 1000\n",
    "        n_en_words = 0\n",
    "        updates = 0\n",
    "\n",
    "        for en_word_tuples in sorted(en_words.items(), key = lambda k:(k[1], k[0]), reverse = True):\n",
    "            en_word = en_word_tuples[0]\n",
    "            n_en_words += 1\n",
    "            if n_en_words > max_words:\n",
    "                break\n",
    "            n_vi_words = 0\n",
    "            for vi_word_tuples in sorted(vi_words.items(), key = lambda k:(k[1], k[0]), reverse = True):\n",
    "                vi_word = vi_word_tuples[0]\n",
    "                n_vi_words += 1\n",
    "                if n_vi_words > max_words:\n",
    "                    break\n",
    "                if (vi_word, en_word) not in count or en_word not in total:\n",
    "                    continue\n",
    "                    # assume in this case: t[(vi_word, en_word)] = uniform\n",
    "                else:\n",
    "                    if abs(t[(vi_word, en_word)] - count[(vi_word, en_word)] / total[en_word]) > 0.005:\n",
    "                        has_converged = False\n",
    "                        max_change = max(max_change, abs(t[(vi_word, en_word)] - count[(vi_word, en_word)] / total[en_word]))\n",
    "                    t[(vi_word, en_word)] = count[(vi_word, en_word)] / total[en_word]\n",
    "\n",
    "    print(\"Iteration \" + str(n_iters) + \" Completed, Maximum Change: \" + str(max_change))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cb655d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3caee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6f7726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53af8650",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc385c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5642fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

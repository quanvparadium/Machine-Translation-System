{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (4.65.0)\n",
      "Requirement already satisfied: fsspec==2023.9.2 in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (2023.9.2)\n",
      "Requirement already satisfied: datasets==2.14.6 in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (2.14.6)\n",
      "Requirement already satisfied: sentencepiece==0.1.97 in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (0.1.97)\n",
      "Requirement already satisfied: sacrebleu==2.3.1 in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (2.3.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from datasets==2.14.6) (13.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from datasets==2.14.6) (0.18.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from datasets==2.14.6) (2.28.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from datasets==2.14.6) (6.0.1)\n",
      "Requirement already satisfied: pandas in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from datasets==2.14.6) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from datasets==2.14.6) (0.70.14)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from datasets==2.14.6) (0.3.6)\n",
      "Requirement already satisfied: aiohttp in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from datasets==2.14.6) (3.8.6)\n",
      "Requirement already satisfied: packaging in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from datasets==2.14.6) (23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from datasets==2.14.6) (1.25.1)\n",
      "Requirement already satisfied: xxhash in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from datasets==2.14.6) (3.4.1)\n",
      "Requirement already satisfied: regex in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from sacrebleu==2.3.1) (2023.10.3)\n",
      "Requirement already satisfied: portalocker in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from sacrebleu==2.3.1) (2.8.2)\n",
      "Requirement already satisfied: lxml in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from sacrebleu==2.3.1) (4.9.3)\n",
      "Requirement already satisfied: colorama in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from sacrebleu==2.3.1) (0.4.6)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from sacrebleu==2.3.1) (0.9.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from aiohttp->datasets==2.14.6) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from aiohttp->datasets==2.14.6) (3.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from aiohttp->datasets==2.14.6) (1.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from aiohttp->datasets==2.14.6) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from aiohttp->datasets==2.14.6) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from aiohttp->datasets==2.14.6) (23.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from aiohttp->datasets==2.14.6) (4.0.3)\n",
      "Requirement already satisfied: filelock in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.6) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.6) (4.6.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.6) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.6) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.6) (2023.7.22)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from pandas->datasets==2.14.6) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from pandas->datasets==2.14.6) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from pandas->datasets==2.14.6) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/vophananhquan/miniforge3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.14.6) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install 'tqdm' 'fsspec==2023.9.2' 'datasets==2.14.6' 'sentencepiece==0.1.97' 'sacrebleu==2.3.1'\n",
    "# !pip install fsspec==2023.9.2\n",
    "# !pip install datasets==2.14.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import copy\n",
    "import heapq\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import sacrebleu\n",
    "\n",
    "import datasets\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = datasets.load_dataset(\"mt_eng_vietnamese\", \"iwslt2015-en-vi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find a dataset script at /Users/vophananhquan/Downloads/Source Code/Machine-Translation-System/template/iwslt2015-en-vi/iwslt2015-en-vi.py or any data file in the same directory. Couldn't find 'iwslt2015-en-vi' on the Hugging Face Hub either: FileNotFoundError: Dataset 'iwslt2015-en-vi' doesn't exist on the Hub. If the repo is private or gated, make sure to log in with `huggingface-cli login`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/vophananhquan/Downloads/Source Code/Machine-Translation-System/template/base.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/vophananhquan/Downloads/Source%20Code/Machine-Translation-System/template/base.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m corpus \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39;49mload_dataset(\u001b[39m\"\u001b[39;49m\u001b[39miwslt2015-en-vi\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/datasets/load.py:2129\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2124\u001b[0m verification_mode \u001b[39m=\u001b[39m VerificationMode(\n\u001b[1;32m   2125\u001b[0m     (verification_mode \u001b[39mor\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m save_infos \u001b[39melse\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2126\u001b[0m )\n\u001b[1;32m   2128\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2129\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   2130\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m   2131\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   2132\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   2133\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   2134\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   2135\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   2136\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   2137\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   2138\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   2139\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   2140\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   2141\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m   2142\u001b[0m )\n\u001b[1;32m   2144\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2145\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/datasets/load.py:1815\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1813\u001b[0m     download_config \u001b[39m=\u001b[39m download_config\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m download_config \u001b[39melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1814\u001b[0m     download_config\u001b[39m.\u001b[39mstorage_options\u001b[39m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 1815\u001b[0m dataset_module \u001b[39m=\u001b[39m dataset_module_factory(\n\u001b[1;32m   1816\u001b[0m     path,\n\u001b[1;32m   1817\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1818\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1819\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1820\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1821\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1822\u001b[0m )\n\u001b[1;32m   1823\u001b[0m \u001b[39m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1824\u001b[0m builder_kwargs \u001b[39m=\u001b[39m dataset_module\u001b[39m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/datasets/load.py:1508\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m                 \u001b[39mraise\u001b[39;00m e1 \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e1, \u001b[39mFileNotFoundError\u001b[39;00m):\n\u001b[0;32m-> 1508\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1509\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find a dataset script at \u001b[39m\u001b[39m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[39m}\u001b[39;00m\u001b[39m or any data file in the same directory. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1510\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m on the Hugging Face Hub either: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(e1)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00me1\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1511\u001b[0m                 ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1512\u001b[0m             \u001b[39mraise\u001b[39;00m e1 \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find a dataset script at /Users/vophananhquan/Downloads/Source Code/Machine-Translation-System/template/iwslt2015-en-vi/iwslt2015-en-vi.py or any data file in the same directory. Couldn't find 'iwslt2015-en-vi' on the Hugging Face Hub either: FileNotFoundError: Dataset 'iwslt2015-en-vi' doesn't exist on the Hub. If the repo is private or gated, make sure to log in with `huggingface-cli login`."
     ]
    }
   ],
   "source": [
    "corpus = datasets.load_dataset(\"iwslt2015-en-vi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 133318\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1269\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1269\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparing:\n",
    "    def __init__(self, save_data_dir, source_lang, target_lang):\n",
    "        self.save_data_dir = save_data_dir\n",
    "        self.source_lang = source_lang\n",
    "        self.target_lang = target_lang\n",
    "    \n",
    "    def download_dataset(self):\n",
    "        if not(os.path.exists(self.save_data_dir)):\n",
    "            print('Create Foler')\n",
    "            os.mkdir(self.save_data_dir)\n",
    "        if len(os.listdir(self.save_data_dir)) ==0:\n",
    "            print('#1-Download Dataset')\n",
    "            corpus = datasets.load_dataset(\"mt_eng_vietnamese\", \"iwslt2015-en-vi\")\n",
    "            \n",
    "            print('#2-Save Dataset')\n",
    "            for data in ['train', 'validation', 'test']:\n",
    "\n",
    "                source_data, target_data = self.get_data(corpus[data])\n",
    "\n",
    "                print('Source lang: {} - {}: {}'.format(self.source_lang, data, len(source_data)))\n",
    "                print('Target lang: {} - {}: {}'.format(self.target_lang, data, len(target_data)))\n",
    "\n",
    "                self.save_data(source_data, os.path.join(self.save_data_dir, data + '.' + self.source_lang))\n",
    "                self.save_data(target_data, os.path.join(self.save_data_dir, data + '.' + self.target_lang))\n",
    "\n",
    "        else:\n",
    "            print('Dataset exit!')\n",
    "        \n",
    "    def get_data(self, corpus):\n",
    "        source_data = []\n",
    "        target_data = []\n",
    "        for data in corpus:\n",
    "            source_data.append(data['translation'][self.source_lang])\n",
    "            target_data.append(data['translation'][self.target_lang])\n",
    "        return source_data, target_data\n",
    "\n",
    "    def save_data(self, data, save_path):\n",
    "        print('=> Save data => Path: {}'.format(save_path))\n",
    "        with open(save_path, 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sentencepiece(cfg, is_src=True):\n",
    "    template = \"--input={} \\\n",
    "                --pad_id={} \\\n",
    "                --bos_id={} \\\n",
    "                --eos_id={} \\\n",
    "                --unk_id={} \\\n",
    "                --model_prefix={} \\\n",
    "                --vocab_size={} \\\n",
    "                --character_coverage={} \\\n",
    "                --model_type={}\"\n",
    "    \n",
    "    if is_src:\n",
    "        train_file = f\"{cfg.data_dir}/train.{cfg.src_lang}\"\n",
    "        model_prefix = f\"{cfg.sp_dir}/{cfg.src_model_prefix}\"\n",
    "    else:\n",
    "        train_file = f\"{cfg.data_dir}/train.{cfg.tgt_lang}\"\n",
    "        model_prefix = f\"{cfg.sp_dir}/{cfg.tgt_model_prefix}\"\n",
    "\n",
    "    print(f\"===> Processing file: {train_file}\")\n",
    "    if not os.path.isdir(cfg.sp_dir):\n",
    "        os.mkdir(cfg.sp_dir)\n",
    "\n",
    "    sp_cfg = template.format(\n",
    "        train_file,\n",
    "        cfg.pad_id,\n",
    "        cfg.sos_id,\n",
    "        cfg.eos_id,\n",
    "        cfg.unk_id,\n",
    "        model_prefix,\n",
    "        cfg.sp_vocab_size,\n",
    "        cfg.character_coverage,\n",
    "        cfg.model_type)\n",
    "    \n",
    "    spm.SentencePieceTrainer.Train(sp_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTDataset(Dataset):\n",
    "    def __init__(self, cfg, data_type=\"train\"):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.sp_src, self.sp_tgt = self.load_sp_tokenizer()\n",
    "        self.src_texts, self.tgt_texts = self.read_data(data_type)\n",
    "\n",
    "        src_tokenized_sequences = self.texts_to_sequences(self.src_texts, True)\n",
    "        tgt_input_tokenized_sequences, tgt_output_tokenized_sequences = self.texts_to_sequences(self.tgt_texts, False)\n",
    "\n",
    "        self.src_data = torch.LongTensor(src_tokenized_sequences)\n",
    "        self.input_tgt_data = torch.LongTensor(tgt_input_tokenized_sequences)\n",
    "        self.output_tgt_data = torch.LongTensor(tgt_output_tokenized_sequences)\n",
    "\n",
    "    def read_data(self, data_type):\n",
    "        print(f\"===> Load data from: {self.cfg.data_dir}/{data_type}.{self.cfg.src_lang}\")\n",
    "        with open(f\"{self.cfg.data_dir}/{data_type}.{self.cfg.src_lang}\", 'r') as f:\n",
    "            src_texts = f.readlines()\n",
    "\n",
    "        print(f\"===> Load data from: {self.cfg.data_dir}/{data_type}.{self.cfg.tgt_lang}\")\n",
    "        with open(f\"{self.cfg.data_dir}/{data_type}.{self.cfg.tgt_lang}\", 'r') as f:\n",
    "            trg_texts = f.readlines()\n",
    "        \n",
    "        return src_texts, trg_texts\n",
    "    \n",
    "    def load_sp_tokenizer(self):\n",
    "        sp_src = spm.SentencePieceProcessor()\n",
    "        sp_src.Load(f\"{self.cfg.sp_dir}/{self.cfg.src_model_prefix}.model\")\n",
    "\n",
    "        sp_tgt = spm.SentencePieceProcessor()\n",
    "        sp_tgt.Load(f\"{self.cfg.sp_dir}/{self.cfg.tgt_model_prefix}.model\")\n",
    "\n",
    "        return sp_src, sp_tgt\n",
    "    \n",
    "    def texts_to_sequences(self, texts, is_src=True):\n",
    "        if is_src:\n",
    "            src_tokenized_sequences = []\n",
    "            for text in tqdm(texts):\n",
    "                tokenized = self.sp_src.EncodeAsIds(text.strip())\n",
    "                src_tokenized_sequences.append(\n",
    "                    pad_or_truncate([self.cfg.sos_id] + tokenized + [self.cfg.eos_id], self.cfg.seq_len, self.cfg.pad_id)\n",
    "                )\n",
    "            return src_tokenized_sequences\n",
    "        else:\n",
    "            tgt_input_tokenized_sequences = []\n",
    "            tgt_output_tokenized_sequences = []\n",
    "            for text in tqdm(texts):\n",
    "                tokenized = self.sp_tgt.EncodeAsIds(text.strip())\n",
    "                tgt_input = [self.cfg.sos_id] + tokenized\n",
    "                tgt_output = tokenized + [self.cfg.eos_id]\n",
    "                tgt_input_tokenized_sequences.append(pad_or_truncate(tgt_input, self.cfg.seq_len, self.cfg.pad_id))\n",
    "                tgt_output_tokenized_sequences.append(pad_or_truncate(tgt_output, self.cfg.seq_len, self.cfg.pad_id))\n",
    "\n",
    "            return tgt_input_tokenized_sequences, tgt_output_tokenized_sequences\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.src_data[idx], self.input_tgt_data[idx], self.output_tgt_data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return np.shape(self.src_data)[0]\n",
    "\n",
    "def pad_or_truncate(tokenized_sequence, seq_len, pad_id):\n",
    "    if len(tokenized_sequence) < seq_len:\n",
    "        left = seq_len - len(tokenized_sequence)\n",
    "        padding = [pad_id] * left\n",
    "        tokenized_sequence += padding\n",
    "    else:\n",
    "        tokenized_sequence = tokenized_sequence[:seq_len]\n",
    "    return tokenized_sequence\n",
    "\n",
    "def get_data_loader(cfg, data_type='train'):\n",
    "    dataset = NMTDataset(cfg, data_type)\n",
    "\n",
    "    if data_type == 'train':\n",
    "        shuffle = True\n",
    "    else:\n",
    "        shuffle = False\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=cfg.batch_size, shuffle=shuffle)\n",
    "\n",
    "    return dataset, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, drop_out=0.1):\n",
    "        super().__init__()\n",
    "        self.inf = 1e9\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        # W^Q, W^K, W^V in the paper\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        self.attn_softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        # Final output linear transformation\n",
    "        self.w_0 = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        input_shape = q.shape\n",
    "\n",
    "        # Linear calculation +  split into num_heads\n",
    "        q = self.w_q(q).view(input_shape[0], -1, self.num_heads, self.d_k) # (B, L, num_heads, d_k)\n",
    "        k = self.w_k(k).view(input_shape[0], -1, self.num_heads, self.d_k) # (B, L, num_heads, d_k)\n",
    "        v = self.w_v(v).view(input_shape[0], -1, self.num_heads, self.d_k) # (B, L, num_heads, d_k)\n",
    "\n",
    "        # For convenience, convert all tensors in size (B, num_heads, L, d_k)\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        # Conduct self-attention\n",
    "        attn_values = self.self_attention(q, k, v, mask=mask) # (B, num_heads, L, d_k)\n",
    "        concat_output = attn_values.transpose(1, 2)\\\n",
    "            .contiguous().view(input_shape[0], -1, self.d_model) # (B, L, d_model)\n",
    "\n",
    "        return self.w_0(concat_output)\n",
    "\n",
    "    def self_attention(self, q, k, v, mask=None):\n",
    "        # Calculate attention scores with scaled dot-product attention\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) # (B, num_heads, L, L)\n",
    "        attn_scores = attn_scores / math.sqrt(self.d_k)\n",
    "\n",
    "        # If there is a mask, make masked spots -INF\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1) # (B, 1, L) => (B, 1, 1, L) or (B, L, L) => (B, 1, L, L)\n",
    "            attn_scores = attn_scores.masked_fill_(mask == 0, -1 * self.inf)\n",
    "\n",
    "        # Softmax and multiplying K to calculate attention value\n",
    "        attn_distribs = self.attn_softmax(attn_scores)\n",
    "\n",
    "        attn_distribs = self.dropout(attn_distribs)\n",
    "        attn_values = torch.matmul(attn_distribs, v) # (B, num_heads, L, d_k)\n",
    "\n",
    "        return attn_values\n",
    "\n",
    "class FeedFowardLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, drop_out=0.1):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model, bias=True)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear_1(x)) # (B, L, d_ff)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear_2(x) # (B, L, d_model)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.layer = nn.LayerNorm([d_model], elementwise_affine=True, eps=self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, seq_len, d_model, device):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "        # Make initial positional encoding matrix with 0\n",
    "        pe_matrix= torch.zeros(seq_len, d_model) # (L, d_model)\n",
    "\n",
    "        # Calculating position encoding values\n",
    "        for pos in range(seq_len):\n",
    "            for i in range(d_model):\n",
    "                if i % 2 == 0:\n",
    "                    pe_matrix[pos, i] = math.sin(pos / (10000 ** (2 * i / d_model)))\n",
    "                elif i % 2 == 1:\n",
    "                    pe_matrix[pos, i] = math.cos(pos / (10000 ** (2 * i / d_model)))\n",
    "\n",
    "        pe_matrix = pe_matrix.unsqueeze(0) # (1, L, d_model)\n",
    "        self.positional_encoding = pe_matrix.to(device=device).requires_grad_(False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * math.sqrt(self.d_model) # (B, L, d_model)\n",
    "        x = x + self.positional_encoding # (B, L, d_model)\n",
    "\n",
    "        return x\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, drop_out=0.1):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = LayerNormalization(d_model)\n",
    "        self.multihead_attention = MultiheadAttention(d_model, num_heads, drop_out)\n",
    "        self.drop_out_1 = nn.Dropout(drop_out)\n",
    "\n",
    "        self.layer_norm_2 = LayerNormalization(d_model)\n",
    "        self.feed_forward = FeedFowardLayer(d_model, d_ff, drop_out)\n",
    "        self.drop_out_2 = nn.Dropout(drop_out)\n",
    "\n",
    "    def forward(self, x, e_mask):\n",
    "        x_1 = self.layer_norm_1(x) # (B, L, d_model)\n",
    "        x = x + self.drop_out_1(\n",
    "            self.multihead_attention(x_1, x_1, x_1, mask=e_mask)\n",
    "        ) # (B, L, d_model)\n",
    "\n",
    "        x_2 = self.layer_norm_2(x) # (B, L, d_model)\n",
    "        x = x + self.drop_out_2(self.feed_forward(x_2)) # (B, L, d_model)\n",
    "\n",
    "        return x # (B, L, d_model)\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, drop_out=0.1):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = LayerNormalization(d_model)\n",
    "        self.masked_multihead_attention = MultiheadAttention(d_model, num_heads, drop_out)\n",
    "        self.drop_out_1 = nn.Dropout(drop_out)\n",
    "\n",
    "        self.layer_norm_2 = LayerNormalization(d_model)\n",
    "        self.multihead_attention = MultiheadAttention(d_model, num_heads, drop_out)\n",
    "        self.drop_out_2 = nn.Dropout(drop_out)\n",
    "\n",
    "        self.layer_norm_3 = LayerNormalization(d_model)\n",
    "        self.feed_forward = FeedFowardLayer(d_model, d_ff, drop_out)\n",
    "        self.drop_out_3 = nn.Dropout(drop_out)\n",
    "\n",
    "    def forward(self, x, e_output, e_mask,  d_mask):\n",
    "        x_1 = self.layer_norm_1(x) # (B, L, d_model)\n",
    "        x = x + self.drop_out_1(\n",
    "            self.masked_multihead_attention(x_1, x_1, x_1, mask=d_mask)\n",
    "        ) # (B, L, d_model)\n",
    "        x_2 = self.layer_norm_2(x) # (B, L, d_model)\n",
    "        x = x + self.drop_out_2(\n",
    "            self.multihead_attention(x_2, e_output, e_output, mask=e_mask)\n",
    "        ) # (B, L, d_model)\n",
    "        x_3 = self.layer_norm_3(x) # (B, L, d_model)\n",
    "        x = x + self.drop_out_3(self.feed_forward(x_3)) # (B, L, d_model)\n",
    "\n",
    "        return x # (B, L, d_model)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, drop_out=0.1):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, drop_out) for i in range(num_layers)]\n",
    "        )\n",
    "        self.layer_norm = LayerNormalization(d_model)\n",
    "\n",
    "    def forward(self, x, e_mask):\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.layers[i](x, e_mask)\n",
    "\n",
    "        return self.layer_norm(x)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, drop_out):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model, num_heads, d_ff, drop_out) for i in range(num_layers)]\n",
    "        )\n",
    "        self.layer_norm = LayerNormalization(d_model)\n",
    "\n",
    "    def forward(self, x, e_output, e_mask, d_mask):\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.layers[i](x, e_output, e_mask, d_mask)\n",
    "\n",
    "        return self.layer_norm(x)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.src_embedding = nn.Embedding(self.cfg.sp_vocab_size, self.cfg.d_model)\n",
    "        self.tgt_embedding = nn.Embedding(self.cfg.sp_vocab_size, self.cfg.d_model)\n",
    "        self.positional_encoder = PositionalEncoder(\n",
    "            self.cfg.seq_len, \n",
    "            self.cfg.d_model, \n",
    "            self.cfg.device\n",
    "        )\n",
    "        self.encoder = Encoder(\n",
    "            self.cfg.num_layers, \n",
    "            self.cfg.d_model, \n",
    "            self.cfg.num_heads, \n",
    "            self.cfg.d_ff, \n",
    "            self.cfg.drop_out\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            self.cfg.num_layers, \n",
    "            self.cfg.d_model, \n",
    "            self.cfg.num_heads, \n",
    "            self.cfg.d_ff, \n",
    "            self.cfg.drop_out\n",
    "        )\n",
    "        self.output_linear = nn.Linear(self.cfg.d_model, self.cfg.sp_vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, src_input, tgt_input, e_mask=None, d_mask=None):\n",
    "        src_input = self.src_embedding(src_input) # (B, L) => (B, L, d_model)\n",
    "        tgt_input = self.tgt_embedding(tgt_input) # (B, L) => (B, L, d_model)\n",
    "        src_input = self.positional_encoder(src_input) # (B, L, d_model) => (B, L, d_model)\n",
    "        tgt_input = self.positional_encoder(tgt_input) # (B, L, d_model) => (B, L, d_model)\n",
    "\n",
    "        e_output = self.encoder(src_input, e_mask) # (B, L, d_model)\n",
    "        d_output = self.decoder(tgt_input, e_output, e_mask, d_mask) # (B, L, d_model)\n",
    "\n",
    "        output = self.softmax(self.output_linear(d_output)) # (B, L, d_model) => # (B, L, trg_vocab_size)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, cfg, is_train=True, load_ckpt=True):\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        print(\"Loading Transformer model & Adam optimizer...\")\n",
    "        self.model = Transformer(self.cfg).to(self.cfg.device)\n",
    "        print(self.cfg.device)\n",
    "\n",
    "        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.cfg.learning_rate)\n",
    "\n",
    "        self.best_loss = 100.0\n",
    "        if load_ckpt:\n",
    "            print(\"Loading checkpoint...\")\n",
    "            checkpoint = torch.load(f\"{self.cfg.ckpt_dir}/{self.cfg.ckpt_name}\", map_location=self.cfg.device)\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.optim.load_state_dict(checkpoint['optim_state_dict'])\n",
    "            self.best_loss = checkpoint['loss']\n",
    "        else:\n",
    "            print(\"Initializing the model...\")\n",
    "            for p in self.model.parameters():\n",
    "                if p.dim() > 1:\n",
    "                    nn.init.xavier_uniform_(p)\n",
    "        \n",
    "        # Prepare Tokenizer\n",
    "        self.prepare_tokenizer()\n",
    "\n",
    "        if is_train:\n",
    "            # Load loss function\n",
    "            print(\"Loading loss function...\")\n",
    "            self.criterion = nn.NLLLoss()\n",
    "\n",
    "            # Load dataloaders\n",
    "            print(\"Loading dataloaders...\")\n",
    "            self.train_dataset, self.train_loader = get_data_loader(self.cfg, 'train')\n",
    "            self.valid_dataset, self.valid_loader = get_data_loader(self.cfg, 'validation')\n",
    "\n",
    "        else:\n",
    "            if os.path.exists(f\"{self.cfg.ckpt_dir}/{self.cfg.ckpt_name}\"):\n",
    "                print(\"Loading sentencepiece tokenizer...\")\n",
    "                self.sp_src = spm.SentencePieceProcessor()\n",
    "                self.sp_tgt = spm.SentencePieceProcessor()\n",
    "                self.sp_src.Load(f\"{self.cfg.sp_dir}/{self.cfg.src_model_prefix}.model\")\n",
    "                self.sp_tgt.Load(f\"{self.cfg.sp_dir}/{self.cfg.tgt_model_prefix}.model\")\n",
    "            else:\n",
    "                print(\"Checkpoint path not exits...\")\n",
    "        \n",
    "        print(\"Setting finished.\")\n",
    "    \n",
    "    def prepare_tokenizer(self):\n",
    "        if not os.path.isdir(self.cfg.sp_dir):\n",
    "            print('Training sentencepiece tokenizer...')\n",
    "            train_sentencepiece(self.cfg, is_src=True)\n",
    "            train_sentencepiece(self.cfg, is_src=False)\n",
    "        else:\n",
    "            print('Tokenization already...')\n",
    "\n",
    "    def train(self):\n",
    "        print(\"Training...\")\n",
    "\n",
    "        for epoch in range(1, self.cfg.num_epochs+1):\n",
    "            print(f\"#################### Epoch: {epoch} ####################\")\n",
    "\n",
    "            self.model.train()\n",
    "            train_losses = []\n",
    "            start_time = datetime.datetime.now()\n",
    "\n",
    "            bar = tqdm(enumerate(self.train_loader), total=len(self.train_loader), desc='TRAINING')\n",
    "\n",
    "            for batch_idx, batch in bar:\n",
    "                src_input, tgt_input, tgt_output = batch\n",
    "                src_input, tgt_input, tgt_output = src_input.to(self.cfg.device), tgt_input.to(self.cfg.device), tgt_output.to(self.cfg.device)\n",
    "\n",
    "                e_mask, d_mask = self.create_mask(src_input, tgt_input)\n",
    "\n",
    "                logits = self.model(src_input, tgt_input, e_mask, d_mask)\n",
    "\n",
    "                self.optim.zero_grad()\n",
    "\n",
    "                loss = self.criterion(\n",
    "                    logits.view(-1, logits.shape[-1]),\n",
    "                    tgt_output.reshape(-1)\n",
    "                )\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "\n",
    "                train_losses.append(loss.item())\n",
    "                \n",
    "                del src_input, tgt_input, tgt_output, e_mask, d_mask, logits\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                bar.set_postfix(TRAIN=\"Epoch {} - Batch_Loss {:.2f} - Train_Loss {:.2f} - Best_Valid_Loss {:.2f}\".format(\n",
    "                    epoch,\n",
    "                    loss.item(),\n",
    "                    np.mean(train_losses),\n",
    "                    self.best_loss\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            end_time = datetime.datetime.now()\n",
    "            training_time = end_time - start_time\n",
    "\n",
    "            mean_train_loss = np.mean(train_losses)\n",
    "            print(f\"Train loss: {mean_train_loss} || Time: {training_time} secs\")\n",
    "\n",
    "            valid_loss, valid_time = self.validation()\n",
    "            \n",
    "            if valid_loss < self.best_loss:\n",
    "                if not os.path.exists(self.cfg.ckpt_dir):\n",
    "                    os.mkdir(self.cfg.ckpt_dir)\n",
    "                    \n",
    "                self.best_loss = valid_loss\n",
    "                state_dict = {\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optim_state_dict': self.optim.state_dict(),\n",
    "                    'loss': self.best_loss\n",
    "                }\n",
    "                torch.save(state_dict, f\"{self.cfg.ckpt_dir}/{self.cfg.ckpt_name}\")\n",
    "                print(f\"***** Current best checkpoint is saved. *****\")\n",
    "\n",
    "            print(f\"Best valid loss: {self.best_loss}\")\n",
    "            print(f\"Valid loss: {valid_loss} || One epoch training time: {valid_time}\")\n",
    "\n",
    "        print(f\"Training finished!\")\n",
    "        \n",
    "    def validation(self):\n",
    "        self.model.eval()\n",
    "        \n",
    "        valid_losses = []\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            bar = tqdm(enumerate(self.valid_loader), total=len(self.valid_loader), desc='VALIDATIION')\n",
    "            for batch_idx, batch in bar:\n",
    "                src_input, tgt_input, tgt_output = batch\n",
    "                src_input, tgt_input, tgt_output = src_input.to(self.cfg.device), tgt_input.to(self.cfg.device), tgt_output.to(self.cfg.device)\n",
    "\n",
    "                e_mask, d_mask = self.create_mask(src_input, tgt_input)\n",
    "\n",
    "                logits = self.model(src_input, tgt_input, e_mask, d_mask)\n",
    "\n",
    "                loss = self.criterion(\n",
    "                    logits.view(-1, logits.shape[-1]),\n",
    "                    tgt_output.reshape(-1)\n",
    "                )\n",
    "\n",
    "                valid_losses.append(loss.item())\n",
    "\n",
    "                bar.set_postfix(TRAIN=\"Batch_Loss {:.2f} - Valid_Loss {:.2f}\".format(\n",
    "                    loss.item(),\n",
    "                    np.mean(valid_losses)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                del src_input, tgt_input, tgt_output, e_mask, d_mask, logits\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        end_time = datetime.datetime.now()\n",
    "        validation_time = end_time - start_time\n",
    "        \n",
    "        mean_valid_loss = np.mean(valid_losses)\n",
    "        \n",
    "        return mean_valid_loss, f\"{validation_time} secs\"\n",
    "\n",
    "    def inference(self, input_sentence):\n",
    "        self.model.eval()\n",
    "\n",
    "        print(\"Preprocessing input sentence...\")\n",
    "        tokenized = self.sp_src.EncodeAsIds(input_sentence)\n",
    "        src_data = torch.LongTensor(\n",
    "            pad_or_truncate([self.cfg.sos_id] + tokenized + [self.cfg.eos_id], self.cfg.seq_len, self.cfg.pad_id)\n",
    "        ).unsqueeze(0).to(self.cfg.device)\n",
    "\n",
    "        e_mask = (src_data != self.cfg.pad_id).unsqueeze(1).to(self.cfg.device) # (1, 1, L)\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        print(\"Encoding input sentence...\")\n",
    "        src_data = self.model.src_embedding(src_data)\n",
    "        src_data = self.model.positional_encoder(src_data)\n",
    "        e_output = self.model.encoder(src_data, e_mask) # (1, L, d_model)\n",
    "\n",
    "        result = self.greedy_search(e_output, e_mask)\n",
    "\n",
    "        end_time = datetime.datetime.now()\n",
    "\n",
    "        total_inference_time = end_time - start_time\n",
    "\n",
    "        print(f\"Input: {input_sentence}\")\n",
    "        print(f\"Result: {result}\")\n",
    "        print(f\"Inference finished! || Total inference time: {total_inference_time}secs\")\n",
    "        return result\n",
    "        \n",
    "    def greedy_search(self, e_output, e_mask):\n",
    "        last_words = torch.LongTensor([self.cfg.pad_id] * self.cfg.seq_len).to(self.cfg.device) # (L)\n",
    "        last_words[0] = self.cfg.sos_id # (L)\n",
    "        cur_len = 1\n",
    "\n",
    "        for i in range(self.cfg.seq_len):\n",
    "            d_mask = (last_words.unsqueeze(0) != self.cfg.pad_id).unsqueeze(1).to(self.cfg.device) # (1, 1, L)\n",
    "            nopeak_mask = torch.ones([1, self.cfg.seq_len, self.cfg.seq_len], dtype=torch.bool).to(self.cfg.device)  # (1, L, L)\n",
    "            nopeak_mask = torch.tril(nopeak_mask)  # (1, L, L) to triangular shape\n",
    "            d_mask = d_mask & nopeak_mask  # (1, L, L) padding false\n",
    "\n",
    "            tgt_embedded = self.model.tgt_embedding(last_words.unsqueeze(0))\n",
    "            tgt_positional_encoded = self.model.positional_encoder(tgt_embedded)\n",
    "            decoder_output = self.model.decoder(\n",
    "                tgt_positional_encoded,\n",
    "                e_output,\n",
    "                e_mask,\n",
    "                d_mask\n",
    "            ) # (1, L, d_model)\n",
    "\n",
    "            output = self.model.softmax(\n",
    "                self.model.output_linear(decoder_output)\n",
    "            ) # (1, L, trg_vocab_size)\n",
    "\n",
    "            output = torch.argmax(output, dim=-1) # (1, L)\n",
    "            last_word_id = output[0][i].item()\n",
    "            \n",
    "            if i < self.cfg.seq_len-1:\n",
    "                last_words[i+1] = last_word_id\n",
    "                cur_len += 1\n",
    "            \n",
    "            if last_word_id == self.cfg.eos_id:\n",
    "                break\n",
    "\n",
    "        if last_words[-1].item() == self.cfg.pad_id:\n",
    "            decoded_output = last_words[1:cur_len].tolist()\n",
    "        else:\n",
    "            decoded_output = last_words[1:].tolist()\n",
    "        decoded_output = self.sp_tgt.decode_ids(decoded_output)\n",
    "        \n",
    "        return decoded_output\n",
    "\n",
    "    def create_mask(self, src_input, tgt_input):\n",
    "        e_mask = (src_input != self.cfg.pad_id).unsqueeze(1)  # (B, 1, L)\n",
    "        d_mask = (tgt_input != self.cfg.pad_id).unsqueeze(1)  # (B, 1, L)\n",
    "\n",
    "        nopeak_mask = torch.ones([1, self.cfg.seq_len, self.cfg.seq_len], dtype=torch.bool)  # (1, L, L)\n",
    "        nopeak_mask = torch.tril(nopeak_mask).to(self.cfg.device)  # (1, L, L) to triangular shape\n",
    "        d_mask = d_mask & nopeak_mask  # (B, L, L) padding false\n",
    "\n",
    "        return e_mask, d_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseConfig:\n",
    "    \"\"\" base Encoder Decoder config \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "class NMTConfig(BaseConfig):\n",
    "    # Dataset\n",
    "    data_dir = './transformer/data'\n",
    "    src_lang = 'vi'\n",
    "    tgt_lang = 'en'\n",
    "\n",
    "    # Tokenizer\n",
    "    sp_dir = data_dir + '/sp'\n",
    "    pad_id = 0\n",
    "    sos_id = 1\n",
    "    eos_id = 2\n",
    "    unk_id = 3\n",
    "    src_model_prefix = 'sp_' + src_lang\n",
    "    tgt_model_prefix = 'sp_' + tgt_lang\n",
    "    sp_vocab_size = 4000\n",
    "    character_coverage = 1.0\n",
    "    model_type = 'unigram'\n",
    "\n",
    "    # Model\n",
    "    num_heads = 8\n",
    "    num_layers = 6\n",
    "    d_model = 512\n",
    "    d_ff = 2048\n",
    "    drop_out = 0.1\n",
    "\n",
    "    # Training\n",
    "    # device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "    device = 'cpu'\n",
    "    learning_rate = 1e-4\n",
    "    batch_size = 256\n",
    "    seq_len = 150\n",
    "    num_epochs = 20\n",
    "    ckpt_dir = './transformer'\n",
    "    ckpt_name = 'best_ckpt.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = NMTConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre = DataPreparing(cfg.data_dir, cfg.src_lang, cfg.tgt_lang)\n",
    "data_pre.download_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(cfg, is_train=True, load_ckpt=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
